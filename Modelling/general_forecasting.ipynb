{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"1\">Contents</a>\n",
    "\n",
    "## [1. Introduction](#intro)\n",
    "## [2. Data Analysis](#data-analysis)\n",
    "## [3. Time Series](#time-series-analysis)\n",
    "### [3.1 Hodrick-Prescott filter](#hodrick-prescott-filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"intro\">1. Introduction</a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to competition, retailers aim to increase profits and reduce costs, increasing the profit margin for perishable food products. This means that avoiding costs due to lost sales, and because of the short-shelf life of their products, ensuring that there is no build up of inventory. Effecient forecasting system can result in reduced inventory, be flexible to changes and increase profits. \n",
    "\n",
    "Time series is a series of data points indexed by time typically in an ordered equally spaced manner. Time serves as the only feature in this format of data, and behavior of the data is analyzed through time. Time series forecasting uses past observations of the same variable to develop a model describing the underlying relationship. The model is then used to extrapolate time series into the future. This approach is useful when there are no other explanatory variables influencing the generation of the underlying data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"data-analysis\">2. Data Analysis</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.filters.hp_filter import hpfilter\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import urllib\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Custom upload with connection string\n",
    "from engine_info import server_info\n",
    "\n",
    "#modules for deep learning with LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "#additional modules\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a connection to AWS RDS\n",
    "params = urllib.parse.quote_plus(server_info)\n",
    "engine = create_engine('mssql+pyodbc:///?odbc_connect=%s' % params)\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload Durban Fresh Produce Market sales data\n",
    "sales = pd.read_sql_table(\n",
    "    table_name='Durban_Fresh_produce_market',\n",
    "    con=connection,\n",
    "    parse_dates=['Date']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"data-overview\">2.1 Data Overview</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first five rows\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View datatype of each column\n",
    "sales.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data type for each column above, some of the columns are not on their appropriate data format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert some of the columns to their appropriate data type\n",
    "float_columns = ['Weight_Kg', 'Low_Price', 'High_Price', 'Average_Price', \n",
    "                 'Sales_Total', 'Total_Kg_Sold', 'Total_Qty_Sold', 'Stock_On_Hand']\n",
    "\n",
    "\n",
    "# Convert the columns to numeric\n",
    "for col in float_columns:\n",
    "    # sales[col] = sales[col].astype(float)\n",
    "    sales[col] = pd.to_numeric(sales[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the number of days in the database\n",
    "print(f\"{sales['Date'].nunique()} days recorded in the database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove days whereby total sales equal 0 because it registers average_price as zero.\n",
    "filtered_sales = sales[sales['Sales_Total'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{filtered_sales['Date'].nunique()} days recorded in the database after removing rows with zero sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore no days were lost due after removing the rows which had items not sold on that particular day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE\n",
    "# Check PINEAPPLE commodity to observe daily sales\n",
    "sales[(sales['Commodities'] == 'PINEAPPLE QUEEN VICTORIA') & (sales['Container'] == 'LM080') & (sales['Province'] == 'NATAL')].sort_values('Date').head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above table, it can be seen that multiple sales of the same product are taking place on the same day, which means that the data has to be consolidated to one day for some of these products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidation of repeated sales in a single day of the same product to one day\n",
    "df = filtered_sales.groupby(['Province', 'Container', 'Size_Grade', 'Weight_Kg', 'Commodities', 'Date']\n",
    "    )[['Low_Price', 'High_Price', 'Sales_Total', 'Total_Qty_Sold', 'Total_Kg_Sold', 'Stock_On_Hand']].agg(\n",
    "        {\n",
    "            'Low_Price':min,\n",
    "            'High_Price':max,\n",
    "            'Sales_Total':sum,\n",
    "            'Total_Qty_Sold':sum,\n",
    "            'Total_Kg_Sold':sum,\n",
    "            'Stock_On_Hand':sum\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to ensure that every row in every column has data.\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average price per kilogram of each item\n",
    "df['avg_price_per_kg'] = round(df['Sales_Total'] / df['Total_Kg_Sold'], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ?????? Filter for one product NEED A HEADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which product is sold on a daily basis\n",
    "day_count = sales['Date'].nunique()\n",
    "df.groupby(['Province', 'Container', 'Size_Grade', 'Weight_Kg', 'Commodities'])['Commodities'].value_counts().apply(lambda x: x / day_count).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, **APPLE GOLDEN DELICIOUS** has been sold for every day that is recorded in the database. For this notebook, this product will be used as a template for developing a forecasting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[\n",
    "    (df['Commodities'] == 'APPLE GOLDEN DELICIOUS') & \n",
    "    (df['Weight_Kg'] == 12.0) &\n",
    "    (df['Size_Grade'] == '1S') &\n",
    "    (df['Container'] == 'EC120') &\n",
    "    (df['Province'] == 'CAPE')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For time series modelling, interest is only on the date and the feature of interest, in this case \"avg_price_per_kg\"\n",
    "price = filtered_df[['Date', 'avg_price_per_kg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = price.plot(figsize=(12,6), title=\"APPLE GOLDEN DELICIOUS\")\n",
    "ax.autoscale(axis='x', tight=True)\n",
    "ax.set(ylabel='R/kg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy so as to add some columns to the copied dataframe and not the original\n",
    "copy_price = price.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Moving Average for different periods\n",
    "copy_price['5-day-SMA'] = copy_price['avg_price_per_kg'].rolling(window=5).mean() # Idealy a week\n",
    "copy_price['5-day-Std'] = copy_price['avg_price_per_kg'].rolling(window=5).std() # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = copy_price[['avg_price_per_kg', '5-day-SMA', '5-day-Std']].plot(figsize=(14,6), title=\"APPLE GOLDEN DELICIOUS\")\n",
    "ax.autoscale(axis='x', tight=True)\n",
    "ax.set(ylabel='R/kg')\n",
    "ax.legend(bbox_to_anchor=(1,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"time-series-analysis\">3. Time Series Analysis</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View of the date index\n",
    "price.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The freq of the index is currently set to None, this will need to be changed to daily, since the frequency of the data is daily. Furthermore, since there is no data available for weekends, the freq has to be set to Business day (Mon-Fri), with a backfill method to account for those days when it is a holiday and no data updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = price.asfreq('B', method='backfill')\n",
    "#price = price.asfreq('B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#price.interpolate(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"hodrick-prescott-filter\">3.1 Hodrick-Prescott filter</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hodrick-Prescott filter is used to get the trend of the data. This approach separates the time-series into a trend component and a cyclical component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_cycle, price_trend = hpfilter(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price['trend'] = price_trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = price[['trend','avg_price_per_kg']].plot(figsize=(12,6), title=\"APPLE GOLDEN DELICIOUS\")\n",
    "ax.autoscale(axis='x', tight=True)\n",
    "ax.set(ylabel='R/kg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del price['trend']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"seasonal-decomposition\">3.2 Seasonal Decomposition</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series decomposition involves the deconstruction of the time series data into the level, trend, seasonal and noise component. The model is assumed to be additive, i.e. value of our variable is given by the summation of it's deconstructed components.\n",
    "<p style=\"text-align: center; font-weight: bold;\">\n",
    "$y(t) = level + trend + seasonality + noise$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 12,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = seasonal_decompose(price['avg_price_per_kg'], model='additive')  \n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"forecasting\">3.3 Forecasting</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Holt - Winters method**\n",
    "\n",
    "Holt - Winters method is a generalized exponential smooothing method that incorporates **trend** and **seasonal** variation in the model. The model makes use of exponential weighting of the coefficients of past observations in order to give more weight to the most recent observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = price.iloc[:-30]\n",
    "test_data = price.iloc[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExponentialSmoothing(train_data['avg_price_per_kg'], trend='add',seasonal='add',seasonal_periods=7) \n",
    "# seasonal_periods=7 for daily data\n",
    "fitted_model = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = fitted_model.forecast(30).rename('Forecast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['avg_price_per_kg'].plot(legend=True, label='TRAIN', figsize=(16,5))\n",
    "test_data['avg_price_per_kg'].plot(legend=True, label='TEST')\n",
    "ax = test_predictions.plot(legend=True, label='PREDICTION', title=\"APPLE GOLDEN DELICIOUS\")\n",
    "ax.set(ylabel=\"R/kg\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_pred = np.sqrt(mean_squared_error(test_data, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autoregressive (AR) model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Holt-Winters method forecasts the variable of interest using a linear combination of predictors. These predictors are the set of level, trend and seasonal predictors. \n",
    "\n",
    "The autoregression model uses a linear combination of past values of the variable. This is a regression equation whereby the variable of interest is regressed against a set of it's lagged values of order $p$.\n",
    "\n",
    "### $y_{t} = c + \\phi_{1}y_{t-1} + \\phi_{2}y_{t-2} + \\dots + \\phi_{p}y_{t-p} + \\varepsilon_{t}$\n",
    "\n",
    "where $c$ is a constant, $\\phi_{1}$ and $\\phi_{2}$ are lag coefficients up to order $p$, and $\\varepsilon_{t}$ is white noise.\n",
    "\n",
    "For example, an <strong>AR(1)</strong> model would follow the formula\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$y_{t} = c + \\phi_{1}y_{t-1} + \\varepsilon_{t}$\n",
    "\n",
    "whereas an <strong>AR(2)</strong> model would follow the formula\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$y_{t} = c + \\phi_{1}y_{t-1} + \\phi_{2}y_{t-2} + \\varepsilon_{t}$\n",
    "\n",
    "and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar_model(data, lags=1):\n",
    "    \"\"\"\n",
    "    Returns an AutoRegressive model specified by the number of lags\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data: pd.Series\n",
    "        A pandas series with a datetime index, and has frequency of the data specied \n",
    "    lags: int\n",
    "        The number of lags that the AutoRegressive model will use\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    An AR model specified by the number of lags\n",
    "    \"\"\"\n",
    "    \n",
    "    model = AR(data)\n",
    "    ar = model.fit(maxlag=lags)\n",
    "    \n",
    "    return ar    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AR(1) model\n",
    "ar1 = ar_model(train_data['avg_price_per_kg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the general format for obtaining predictions\n",
    "start=len(train_data)\n",
    "end=len(train_data)+len(test_data)-1\n",
    "predictions1 = ar1.predict(start=start, end=end, dynamic=False).rename('AR(1) Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for scoring each of the models\n",
    "scores = pd.DataFrame(columns=[\"RMSE\"])\n",
    "scores.index.name = \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.loc['AR(1)'] = np.sqrt(mean_squared_error(test_data, predictions1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AR(2) model\n",
    "ar2 = ar_model(train_data['avg_price_per_kg'], lags=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = ar2.predict(start=start, end=end, dynamic=False).rename('AR(2) Predictions')\n",
    "scores.loc['AR(2)'] = np.sqrt(mean_squared_error(test_data, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['avg_price_per_kg'].plot(legend=True)\n",
    "predictions1.plot(legend=True)\n",
    "predictions2.plot(legend=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.sort_values(by=\"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results, it can be seen that as the lags added increase, the RMSE is decreasing. Moreover, one needs to determine at what lag will the RMSE reach a minimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_rmse = []\n",
    "for i in range(1, 30): # 30 is an arbitrary number\n",
    "    ar = ar_model(train_data['avg_price_per_kg'], lags=i)\n",
    "    price_pred = ar.predict(start=start, end=end, dynamic=False)\n",
    "    print(price_pred, test_data)\n",
    "    ar_rmse.append(np.sqrt(mean_squared_error(test_data, price_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 30), ar_rmse);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AR(5) model\n",
    "ar5 = ar_model(train_data['avg_price_per_kg'], lags=5)\n",
    "predictions5 = ar5.predict(start=start, end=end, dynamic=False).rename('AR(5) Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.loc['AR(5)'] = np.sqrt(mean_squared_error(test_data, predictions5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['avg_price_per_kg'].plot(legend=True)\n",
    "predictions1.plot(legend=True)\n",
    "predictions2.plot(legend=True)\n",
    "predictions5.plot(legend=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best AR() model to use for forecasting\n",
    "model = AR(train_data['avg_price_per_kg'])\n",
    "arfit = model.fit(maxiter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arfit.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AR(15) model\n",
    "ar15 = ar_model(train_data['avg_price_per_kg'], lags=15)\n",
    "predictions15 = ar15.predict(start=start, end=end, dynamic=False).rename('AR(15) Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.loc['AR(15)'] = np.sqrt(mean_squared_error(test_data, predictions15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['avg_price_per_kg'].plot(legend=True)\n",
    "predictions5.plot(legend=True)\n",
    "predictions15.plot(legend=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AR(16) model\n",
    "ar16 = ar_model(train_data['avg_price_per_kg'], lags=16)\n",
    "predictions16 = ar16.predict(start=start, end=end, dynamic=False).rename('AR(16) Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.loc['AR(16)'] = np.sqrt(mean_squared_error(test_data, predictions16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.sort_values(by=\"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAYBE MAKE A PLOT OF HOW THE GRAPH LOOKS AS YOU CHANGE THE LAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autoregressive Integrated Moing Average (ARIMA) model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA model is a combination of two models, the AR model utilizing past values of the time series data, and the Moving Average (MA) model, which uses past values of the forecast errors. \n",
    "\n",
    "### $$ y_{t} = c + \\sum^p_{i=1} \\phi_{i} y_{t-i} + \\sum^q_{j=1} \\theta_{j} \\varepsilon_{t-j} + \\varepsilon_{t} $$\n",
    "\n",
    "As seen earlier, this models can be also be used separately, or in this section, combined. The fitting process returns estimated coefficients, $\\phi_{i}$ and $\\theta_{i}$, but prior to this process, the order ($p,q$) of the model needs to be determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = auto_arima(train_data['avg_price_per_kg'],error_action='ignore', suppress_warnings=True, start_p=0, start_q=0,\n",
    "                          max_p=6, max_q=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(train_data['avg_price_per_kg'],order=(1,1,1))\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = len(train_data)\n",
    "end = len(train_data) + len(test_data) - 1\n",
    "predictions = results.predict(start=start, end=end, typ='levels').rename(\"ARIMA(1,1,1) predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = pd.Series(predictions, index=test_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 12,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['avg_price_per_kg'].plot(legend=True)\n",
    "predictions15.plot(legend=True)\n",
    "predictions.plot(legend=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.loc['ARIMA(1,1,1)'] = np.sqrt(mean_squared_error(test_data, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.sort_values(by=\"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still needs to be looked at\n",
    "- Granger Causality Test\n",
    "- Vector AutoRegression (VAR) methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"regression\">4. Regression</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='reg-data-analysis'>4.1 Data Analysis</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder of how the dataFrame looks\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['low_price_per_kg'] = round(df['Low_Price'] / df['Weight_Kg'], 2)\n",
    "df['high_price_per_kg'] = round(df['High_Price'] / df['Weight_Kg'], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For time series analysis, the following format was followed when filtering the data:\n",
    "```python\n",
    "    filtered_df = df[\n",
    "    (df['Commodities'] == 'APPLE GOLDEN DELICIOUS') & \n",
    "    (df['Weight_Kg'] == 12.0) &\n",
    "    (df['Size_Grade'] == '1S') &\n",
    "    (df['Container'] == 'EC120') &\n",
    "    (df['Province'] == 'CAPE')\n",
    "    ]\n",
    "```\n",
    "For regression, 'Province' will be excluded since where the product is from might affect the pricing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[\n",
    "    (df['Commodities'] == 'APPLE GOLDEN DELICIOUS') & \n",
    "    (df['Weight_Kg'] == 12.0) &\n",
    "    (df['Size_Grade'] == '1S') &\n",
    "    (df['Container'] == 'EC120') \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apples = filtered_df[[\n",
    "    'Province', 'Date', 'Low_Price', 'High_Price', 'Sales_Total', 'Total_Qty_Sold',\n",
    "    'Total_Kg_Sold', 'Stock_On_Hand', 'avg_price_per_kg', 'low_price_per_kg', 'high_price_per_kg'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check for multicollinearity, only numerical columns can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for multicollinearity\n",
    "sns.heatmap(apples.corr(), annot=True, cbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a high correlation amongst these three features **Sales_Total, Total_Qty_Sold** and **Total_Kg_Sold**. For the sake of determining inventory levels, only **Total_Qty_Sold** will remain. Furthermore there is also perfect correlation between **low_price_per_kg** and **Low_Price**, as well as between **high_price_per_kg** and **High_Price**. Since the target variable is in per kilogram terms, per kilograms values will remain. Although, there seems to be a high correlation between **avg_price_per_kg** and **low_price_per_kg**, for regression analysis the lagged values of the features are going to be used to predict the target variable, hence once the lag has been determined, correlations with the target variables will be assessed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apples.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High correlation columns shall be removed \n",
    "rem_col = ['Sales_Total', 'Total_Kg_Sold', 'Low_Price', 'High_Price']\n",
    "# The remaining columns after removing correlated columns\n",
    "cols = [col for col in apples.columns if col not in rem_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apples_df = apples[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apples_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='feature-engineering'>4.2 Feature Engineering and Visualizations</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apples_df['Province'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the frequency of purchases from each province, apples from ORANGE FREE STATE are the least regularly bought, one option is to remove these rows, another option is to combine it with TRANSVAAL and have them renamed as \"INLAND\". The latter option is the prefered since it means no data is lost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apples_df['Province'] = apples_df['Province'].apply(lambda x: x if x not in [\"TRANSVAAL\", \"ORANGE FREE STATE\"] else \"INLAND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apples_df['Province'] .value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_swarmplot(data_frame, x, y):\n",
    "    \"\"\"\n",
    "    Returns swarmplot based on variables of interest\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data_frame: DataFrame\n",
    "        A DataFrame containing x and y variables\n",
    "    x, y: str\n",
    "        Features of interest in the DataFrame, x and\n",
    "        y plotted on the x-axis and y-axis respectively\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "        A seaborn graph object\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(16,5))\n",
    "    sns.swarmplot(x=x, y=y, data=data_frame)\n",
    "    plt.title(\"Price variations of Apples Golden Delicious\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_swarmplot(apples_df, 'Province', 'avg_price_per_kg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apples_df['weekday'] = apples_df['Date'].apply(lambda x: x.day_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apples_df['month'] = apples_df['Date'].apply(lambda x: x.month_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def season(month):\n",
    "    \"\"\"\n",
    "    Returns the season of which the month falls in\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    month: str\n",
    "        The month of the year as a full month name\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    str:\n",
    "        The season of the year\n",
    "        \n",
    "    Examples\n",
    "    ---------\n",
    "    >>> season('October')\n",
    "    'spring'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Seasons\n",
    "    summer = ['December', 'January', 'February']\n",
    "    autumn = ['March', 'April', 'May']\n",
    "    winter = ['June', 'July', 'August']\n",
    "    spring = ['September', 'October', 'November']\n",
    "    \n",
    "    if month in summer:\n",
    "        return 'summer'\n",
    "    elif month in autumn:\n",
    "        return 'autumn'\n",
    "    elif month in winter:\n",
    "        return 'winter'\n",
    "    else:\n",
    "        return 'spring'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apples_df['season'] = apples_df['month'].apply(season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_swarmplot(apples_df, 'season', 'avg_price_per_kg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this plot, it can be concluded that since apples are not seasonal fruits, there appears to be no difference in average price per kilogram between the seasons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apples_df.sort_values('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apples_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if end of the month(25th - 31st) will influence the prices\n",
    "apples_df['is_month_end'] = apples_df['Date'].apply(lambda x: 1 if x.day in range(25,32) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove white spaces in the Province name\n",
    "apples_df['Province'] = apples_df['Province'].apply(lambda x: x.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\".\", \"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apples_df['Province'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = list(filter(lambda x: x != 'avg_price_per_kg', apples_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = apples_df[X_features]\n",
    "y = apples_df['avg_price_per_kg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the base model, certain features have to be dropped because they are only recorded after the product has been sold for that day, whereas prediction are based on what the average price per kilogram is going to be before any transaction has taken place. These columns that are going to be dropped include **low_price_per_kg**, **high_price_per_kg**, **Total_Qty_Sold** and **Stock_On_Hand**. Although these columns are dropped, their lagged values might serve as an input, that can be looked at later on. Furthermore, the **Date** has been used to generate features, hence it will also be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to serve as input for the regression model\n",
    "lst = [col for col in X.columns if col not in [\n",
    "    'low_price_per_kg', 'high_price_per_kg', 'Stock_On_Hand', 'Date', 'Total_Qty_Sold'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since time has been removed as a sequential feature, splitting the data in accordance with time is not needed. Therefore for the train_test_split, shuffle can still remain at True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, comparison is going to be between a linear regression model, and a constant average price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_predict = np.ones(shape=(len(y_test),)) * y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pred = np.sqrt(mean_squared_error(y_test, mean_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.loc['base_pred'] = base_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_rmse = np.sqrt(mean_squared_error(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(y_test, predict, '.')\n",
    "plt.plot(y_test, y_test, 'r')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title(f'Linear regression RMSE = {reg_rmse}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores.loc['linear_regression'] = reg_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.sort_values(by='RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"deep-learning\">5. Deep Learning</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For deep learning, a Long Short Term Memory (**LSTM**) was used to generate forecasts. LSTM is a special kind of recurrent neural network that is capable of learning long term dependencies in data. This is achieved because the recurring module of the model has a combination of four layers interacting with each other.\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating scaler to scale data between the range or (0,1)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_data)\n",
    "train_data_scaled = scaler.transform(train_data)\n",
    "test_data_scaled = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_scaled[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a time series generator from keras for our scaled train and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TimeseriesGenerator(train_data_scaled, train_data_scaled, length=15, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(150, activation='relu', input_shape=(15, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the model\n",
    "model.fit_generator(generator, epochs=40, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss = model.history.history['loss']\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(range(1, len(model_loss)+1), model_loss) \n",
    "plt.ylabel(\"mean squared error\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.title(\"LSTM model performance\")\n",
    "plt.autoscale(axis='x', tight=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a certain number of epochs the loss starts to converge to a certain value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model to predict the average price per KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]\n",
    "reshaped_data=np.reshape(train_data_scaled[-15:],(1, 15, 1))\n",
    "for i in range(len(test_data_scaled)):\n",
    "    prediction=model.predict(reshaped_data)[0]\n",
    "    output.append(prediction)\n",
    "    reshaped_data=np.append(reshaped_data[:,1:,:],[[prediction]],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = scaler.inverse_transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_rmse = np.sqrt(mean_squared_error(test_data, output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['avg_price_per_kg'].plot(figsize = (16,5), legend=True)\n",
    "ax = output.plot(legend = True)\n",
    "ax.set(ylabel=\"R/kg\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COnclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data input has to be in equal intervals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
